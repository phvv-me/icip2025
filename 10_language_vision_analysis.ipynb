{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OLMOE\n",
    "- Llama3.2-Vision\n",
    "- Phi3.5-Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from flags.lrh import FlagUnembeddingRepresentation\n",
    "from flags.models import VisionLanguageHuggingFaceModel\n",
    "from flags.nlp.synsets import SupportedLanguages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ec45d5ec2f49a7a3f594ace3938375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-01 15:35:03.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflags.models.hf.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mLoaded model: meta-llama/Llama-3.2-11B-Vision-Instruct\u001b[0m\n",
      "\u001b[32m2024-11-01 15:35:03.592\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mflags.models.hf.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m93\u001b[0m - \u001b[33m\u001b[1mmemory cost: 20351 Mb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fur = FlagUnembeddingRepresentation.from_model_id(\n",
    "    id=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    model_cls=VisionLanguageHuggingFaceModel,\n",
    "    language_codes=SupportedLanguages.Llama,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1278)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Woman_at_Lover%27s_Bridge_Tanjung_Sepat_%28cropped%29.jpg/1024px-Woman_at_Lover%27s_Bridge_Tanjung_Sepat_%28cropped%29.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"What can you say about it?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>['What can you say about it?']<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The image depicts a woman standing in front of a body of water, possibly a lake or sea, with a gray and blurry background. The purpose of the image\n"
     ]
    }
   ],
   "source": [
    "out = fur.model.generate(\n",
    "    image=image,\n",
    "    text=sentences,\n",
    "    max_new_tokens=32,\n",
    ")\n",
    "\n",
    "print(fur.model.processor.batch_decode(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 14869976-n in '14869976-n\tcmn:lemma\t污点\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 14869977-n in '14869977-n\tcmn:lemma\t小斑\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15168570-n in '15168570-n\tcmn:lemma\t规定的睡觉时间\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171146-n in '15171146-n\tcmn:lemma\t节日\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171147-n in '15171147-n\tcmn:lemma\t纪念日\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171739-n in '15171739-n\tcmn:lemma\t竞技状态不佳的日子\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171858-n in '15171858-n\tcmn:lemma\t存取时间\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15172882-n in '15172882-n\tcmn:lemma\t选举日\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15173065-n in '15173065-n\tcmn:lemma\t教会年\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15176162-n in '15176162-n\tcmn:lemma\t雾月\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15177867-n in '15177867-n\tcmn:lemma\t希伯来历\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15178842-n in '15178842-n\tcmn:lemma\t回历\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01498548-a in '01498548-a\thrv:lemma\tamoralan\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01498548-a in '01498548-a\thrv:lemma\tnemoralan\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01505508-a in '01505508-a\thrv:lemma\tmnogo više\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01505508-a in '01505508-a\thrv:lemma\tpuno više\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tizuzev\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tizuzevši\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tizuzimajući\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tosim\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02917945-a in '02917945-a\thrv:lemma\tmahunast\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 03202339-n in '03202339-n\thrv:lemma\tmodne potrepštine\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: glg: invalid offset 15300653-n in '15300653-n\tlemma\tmétopa\n",
      "'\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisionLanguageHuggingFaceModel' object has no attribute 'make_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquick_generate_with_topk_guide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguide\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwoman.n.01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmale.n.01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_lemmas_per_synset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_token_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/flags/lrh/flag_representation.py:561\u001b[0m, in \u001b[0;36mFlagUnembeddingRepresentation.quick_generate_with_topk_guide\u001b[0;34m(self, sentences, guide, min_lemmas_per_synset, max_token_count, *args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m concept_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_concept(guide[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39mcargs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(guide) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    559\u001b[0m guide \u001b[38;5;241m=\u001b[39m concept_A \u001b[38;5;241m-\u001b[39m concept_B \u001b[38;5;28;01mif\u001b[39;00m concept_B \u001b[38;5;28;01melse\u001b[39;00m concept_A\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_topk_guide\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/flags/lrh/flag_representation.py:376\u001b[0m, in \u001b[0;36mFlagUnembeddingRepresentation.generate_with_topk_guide\u001b[0;34m(self, input_text, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m text, probe \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(batchedlist(input_text, batch_size)):\n\u001b[0;32m--> 376\u001b[0m     batch_text, batch_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_topk_guide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     text\u001b[38;5;241m.\u001b[39mextend(batch_text)\n\u001b[1;32m    380\u001b[0m     probe\u001b[38;5;241m.\u001b[39mappend(batch_probe)\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/flags/lrh/flag_representation.py:337\u001b[0m, in \u001b[0;36mFlagUnembeddingRepresentation._generate_with_topk_guide\u001b[0;34m(self, input_text, guide, k, steps)\u001b[0m\n\u001b[1;32m    333\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_text)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# inputs = self.model.make_input(input_text, padding=True)[\"input_ids\"]\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_input\u001b[49m(input_text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    338\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_candidates(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], k)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# notice this approach uses a single pass through the model\u001b[39;00m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m#  at the expense of having k times more candidates being processes.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m#  The other option is to use double pass, but takes longer.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m#  On larger models, our bottleneck is time, not memory,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m#  so we choose this approach for now.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/pydantic/main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionLanguageHuggingFaceModel' object has no attribute 'make_input'"
     ]
    }
   ],
   "source": [
    "fur.quick_generate_with_topk_guide(\n",
    "    sentences,\n",
    "    guide=[\"woman.n.01\", \"male.n.01\"],\n",
    "    min_lemmas_per_synset=3,\n",
    "    max_token_count=3,\n",
    "    k=2,\n",
    "    steps=16,\n",
    ")[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
