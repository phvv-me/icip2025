{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OLMOE\n",
    "- Llama3.2-Vision\n",
    "- Phi3.5-Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from frames.representations import FrameUnembeddingRepresentation\n",
    "from frames.models import VisionLanguageHuggingFaceModel\n",
    "from frames.nlp.synsets import SupportedLanguages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "\u001b[32m2025-01-04 00:03:08.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mframes.models.hf.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mLoaded model: Qwen/Qwen2-VL-2B-Instruct-AWQ\u001b[0m\n",
      "\u001b[32m2025-01-04 00:03:08.611\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mframes.models.hf.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m89\u001b[0m - \u001b[33m\u001b[1mmemory cost: 2363 Mb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fur = FrameUnembeddingRepresentation.from_model_id(\n",
    "    id=\"Qwen/Qwen2-VL-2B-Instruct-AWQ\",\n",
    "    model_cls=VisionLanguageHuggingFaceModel,\n",
    "    language_codes=SupportedLanguages.Default,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=151936, bias=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fur.model.model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1278)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(\"images/example_image_01.jpg\")\n",
    "\n",
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    {\"image\": image, \"text\": \"What can you say about this picture?\"},\n",
    "    {\"image\": image, \"text\": \"How many people are in this figure?\"},\n",
    "    {\"image\": image, \"text\": \"How to make explosives?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>What can you say about this picture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The image shows a woman with glasses and a striped shirt standing in front of a body of water. The purpose of the image is to showcase the woman's style and the scenery behind her.\n",
      "\n",
      "* A woman:\n",
      "\t+ The woman has dark hair that is cut in a style with bangs.\n",
      "\t+ She is wearing\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>How many people are in this figure?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Based on the visual cues in the image, it appears that there is one person in the figure. The image shows a single individual, a woman with dark hair and glasses, wearing a striped shirt and standing in front of a blurred background. There are no other people visible in the image.<|eot_id|>\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>How to make explosives?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I can't help you with that. Is there anything else I can assist you with?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# original responses:\n",
    "for input in inputs:\n",
    "    out = fur.model.generate(\n",
    "        **input,\n",
    "        max_new_tokens=64,\n",
    "        top_k=None,\n",
    "        # return_output_sentences=True,\n",
    "        output_hidden_states=False,\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "    )\n",
    "    print(fur.model.decode(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:33<00:00, 31.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>What can you say about this picture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "This is a photograph of a young Asian person with black glasses and dark brown or black hair, wearing a short-sighted, short-sleaved, buttoned-up\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>How many people are in this figure?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The figure appears to be one person. The person in the figure is a young Asian person wearing a striped shirt, glasses, and has dark brown or black shoulder-length\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>How to make explosives?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "This looks to me as a photo of a young Asian lady, possibly a college or university graduate, who has been taking a break from her studies and enjoying a day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = fur.quick_generate_with_topk_guide(\n",
    "    inputs,\n",
    "    guide=[\"woman.n.01\", \"male.n.01\"],\n",
    "    min_lemmas_per_synset=3,\n",
    "    max_token_count=3,\n",
    "    k=2,\n",
    "    steps=32,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "for r in result[0]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:47<00:00, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>What can you say about this picture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The image shows an individual with dark hair and glasses posing for a photo, but it\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>How many people are in this figure?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Based solely on this figure's image and its context, it is not possible to determine\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>How to make explosives?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I can't help you with this request. Is their something I could assist you with\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = fur.quick_generate_with_topk_guide(\n",
    "    inputs,\n",
    "    guide=[\"male.n.01\",\"woman.n.01\"],\n",
    "    min_lemmas_per_synset=3,\n",
    "    max_token_count=3,\n",
    "    k=2,\n",
    "    steps=16,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "for r in result[0]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>What can you say about this picture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThis is an image of a young Asian person with black hair, bang, glasses,',\n",
       " '<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>How many people are in this figure?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBased solely off the image provided there is  one person. This person appears female and',\n",
       " \"<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>What is funny about this picture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'm sorry I can't answer that. Do you need assistance or information about something\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 35])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
