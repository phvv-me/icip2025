{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Words as Frames\n",
    "\n",
    "The Frame Representation Hypothesis consists of assuming words are naturally represented as frames.\n",
    "\n",
    "One way to verify this claim is by checking if the word matrix representation is naturally full-rank. We can measure this by computing the ratio between the matrix rank and the number of vectors in the matrix.\n",
    "\n",
    "In this experiment, we also show words are close to orthogonal, although this is an approximation because the longer the word becomes (higher token count), the less likely it is to be orthogonal. However, since most words have a low token count, the average word orthogonality is quite high, which is a good indicator that words are naturally represented as orthogonal frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from frames.nlp.synsets import SupportedLanguages\n",
    "from frames.representations import FrameUnembeddingRepresentation\n",
    "from frames.utils.memory import gc_cuda\n",
    "from frames.utils.settings import load_models\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = load_models()\n",
    "\n",
    "X = \"token count\"\n",
    "X_MAX = 16\n",
    "\n",
    "HUE1 = \"Model Family\"\n",
    "\n",
    "HUE2 = \"Model\"\n",
    "Y = \"Relative Rank (Matrix Rank / Num Tokens)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52cd4da8851466c87602285842e0246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-06 14:38:27.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mframes.models.hf.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mLoaded model: hugging-quants/Meta-Llama-3.1-70B-BNB-NF4-BF16\u001b[0m\n",
      "\u001b[32m2024-12-06 14:38:27.824\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mframes.models.hf.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m89\u001b[0m - \u001b[33m\u001b[1mmemory cost: 36650 Mb\u001b[0m\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 14869976-n in '14869976-n\tcmn:lemma\t污点\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 14869977-n in '14869977-n\tcmn:lemma\t小斑\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15168570-n in '15168570-n\tcmn:lemma\t规定的睡觉时间\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171146-n in '15171146-n\tcmn:lemma\t节日\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171147-n in '15171147-n\tcmn:lemma\t纪念日\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171739-n in '15171739-n\tcmn:lemma\t竞技状态不佳的日子\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15171858-n in '15171858-n\tcmn:lemma\t存取时间\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15172882-n in '15172882-n\tcmn:lemma\t选举日\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15173065-n in '15173065-n\tcmn:lemma\t教会年\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15176162-n in '15176162-n\tcmn:lemma\t雾月\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15177867-n in '15177867-n\tcmn:lemma\t希伯来历\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: cmn: invalid offset 15178842-n in '15178842-n\tcmn:lemma\t回历\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01498548-a in '01498548-a\thrv:lemma\tamoralan\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01498548-a in '01498548-a\thrv:lemma\tnemoralan\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01505508-a in '01505508-a\thrv:lemma\tmnogo više\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 01505508-a in '01505508-a\thrv:lemma\tpuno više\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tizuzev\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tizuzevši\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tizuzimajući\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02002046-a in '02002046-a\thrv:lemma\tosim\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 02917945-a in '02917945-a\thrv:lemma\tmahunast\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: hrv: invalid offset 03202339-n in '03202339-n\thrv:lemma\tmodne potrepštine\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2214: UserWarning: glg: invalid offset 15300653-n in '15300653-n\tlemma\tmétopa\n",
      "'\n",
      "  warnings.warn(\n",
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "  0%|          | 0/136 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.91 GiB. GPU 1 has a total capacity of 23.59 GiB of which 1.76 GiB is free. Including non-PyTorch memory, this process has 21.60 GiB memory in use. Of the allocated memory 21.28 GiB is allocated by PyTorch, and 71.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m MODELS\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43m[\u001b[49m\u001b[43mget_tokenization_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     23\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m MODELS\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mget_tokenization_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m df])\n\u001b[1;32m     23\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mget_tokenization_data\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m cols \u001b[38;5;241m=\u001b[39m {HUE1: fur\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfamily, HUE2: \u001b[38;5;28mstr\u001b[39m(fur\u001b[38;5;241m.\u001b[39mmodel)}\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m     fur\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_dataframe(fur\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtokenizer, max_token_count\u001b[38;5;241m=\u001b[39mX_MAX)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcols)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m df[Y] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mfur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_orthogonality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/frame.py:444\u001b[0m, in \u001b[0;36mcompute_orthogonality\u001b[0;34m(self, tokens, batch_size)\u001b[0m\n\u001b[1;32m    442\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    443\u001b[0m all_batches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(tokens, num_batches)\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_relative_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/frame.py:444\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    442\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    443\u001b[0m all_batches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(tokens, num_batches)\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_relative_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(all_batches)])\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/frame.py:439\u001b[0m, in \u001b[0;36m_orthogonality\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_relative_rank\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the frame's matrix rank / num vectors\"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Frame(tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_word_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmT\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mrelative_rank\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/frame.py:65\u001b[0m, in \u001b[0;36mFrameUnembeddingRepresentation._make_word_frames\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_word_frames\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Union[List[\u001b[38;5;28mint\u001b[39m], pd\u001b[38;5;241m.\u001b[39mSeries]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    Create word frames from tokens.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Tensor of word frames.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_representations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/unembedding.py:117\u001b[0m, in \u001b[0;36mLinearUnembeddingRepresentation.get_token_representations\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_representations\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    Get token representations for given token IDs.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m        Tensor: Token representations for the given tokens.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_representations\u001b[49m[tokens]\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/unembedding.py:86\u001b[0m, in \u001b[0;36mLinearUnembeddingRepresentation._token_representations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_token_representations\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Compute token representations.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m        Tensor: Token representations.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     reprs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_euclidean_representations\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meaningless\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_token_representation_options(reprs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/unembedding.py:66\u001b[0m, in \u001b[0;36mLinearUnembeddingRepresentation._euclidean_representations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_euclidean_representations\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    Compute Euclidean representations of the unembedding matrix.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m        Tensor: Euclidean representations.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munembedding_matrix\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inv_sqrt_cov_unembedding_matrix\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/Documents/flag-representation-hypothesis/frames/representations/unembedding.py:159\u001b[0m, in \u001b[0;36mLinearUnembeddingRepresentation.unembedding_matrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munembedding_matrix\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Get the unembedding matrix from the model.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        Tensor: Unembedding matrix.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munembedding_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 1 has a total capacity of 23.59 GiB of which 1.76 GiB is free. Including non-PyTorch memory, this process has 21.60 GiB memory in use. Of the allocated memory 21.28 GiB is allocated by PyTorch, and 71.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_tokenization_data(**kwargs):\n",
    "    with gc_cuda():\n",
    "        fur = FrameUnembeddingRepresentation.from_model_id(\n",
    "            language_codes=SupportedLanguages.Llama,\n",
    "            synsets_kwargs=dict(variations=None),\n",
    "            **kwargs,\n",
    "        )\n",
    "        cols = {HUE1: fur.model.family, HUE2: str(fur.model)}\n",
    "        df = (\n",
    "            fur.data.get_dataframe(fur.model.tokenizer, max_token_count=X_MAX)\n",
    "            .assign(**cols)\n",
    "            .to_pandas()\n",
    "        )\n",
    "        df[Y] = (\n",
    "            fur.compute_relative_rank(df[\"tokens\"], batch_size=1 << 12).cpu().numpy()\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "df = MODELS.to_dict(orient=\"index\").values()\n",
    "df = pd.concat([get_tokenization_data(**kwargs) for kwargs in df])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df[df[X] < X_MAX - 1].drop_duplicates([\"lemma\", HUE1])\n",
    "\n",
    "df_hist.to_pickle(\"resources/01_token_count_per_model_family.pkl\")\n",
    "\n",
    "g = sns.displot(\n",
    "    df_hist,\n",
    "    x=X,\n",
    "    hue=HUE1,\n",
    "    kind=\"hist\",\n",
    "    binwidth=1,\n",
    "    shrink=1,\n",
    "    palette=\"colorblind\",\n",
    "    multiple=\"dodge\",\n",
    "    height=5,\n",
    "    aspect=1.1,\n",
    "    facet_kws=dict(legend_out=False),\n",
    ")\n",
    "\n",
    "descriptive_stats = df.groupby(HUE1)[X].describe()\n",
    "\n",
    "# Extract colors used in the histogram\n",
    "palette = sns.color_palette(n_colors=len(descriptive_stats.index))\n",
    "model_families = descriptive_stats.index\n",
    "\n",
    "for color, model_family in zip(palette, model_families):\n",
    "    percentile_75 = descriptive_stats.loc[model_family, \"75%\"]\n",
    "\n",
    "    # Add dashed vertical line\n",
    "    g.ax.axvline(percentile_75, color=color, linestyle=\"--\", linewidth=2)\n",
    "\n",
    "dashed_line = mlines.Line2D(\n",
    "    [], [], color=\"black\", linestyle=\"--\", linewidth=1.5, label=\"75% Percentile\"\n",
    ")\n",
    "\n",
    "legend_data = {\n",
    "    model_family: plt.Line2D([], [], color=color, marker=\"o\", linestyle=\"\")\n",
    "    for model_family, color in zip(model_families, palette)\n",
    "}\n",
    "legend_data[\"75% Percentile\"] = dashed_line\n",
    "\n",
    "g.add_legend(legend_data=legend_data, title=HUE1)\n",
    "\n",
    "plt.xticks(range(1, X_MAX - 1))\n",
    "\n",
    "plt.savefig(\n",
    "    \"resources/01_token_count_per_model_family.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineplot = (\n",
    "    df.drop_duplicates([\"lemma\", HUE2])\n",
    "    .groupby([\"lemma\", HUE2, X])[Y]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_lineplot.to_pickle(\"resources/02_tokenization_frames.pkl\")\n",
    "\n",
    "sns.lineplot(\n",
    "    df_lineplot,\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    hue=HUE2,\n",
    "    style=HUE2,\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette=\"colorblind\",\n",
    ")\n",
    "\n",
    "plt.xticks(range(1, X_MAX))\n",
    "\n",
    "plt.savefig(\"resources/02_tokenization_frames.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
