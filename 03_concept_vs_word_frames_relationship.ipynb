{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Words as Frames\n",
    "\n",
    "The Frame Representation Hypothesis assumes we can build concepts by \"averaging\" word frames.\n",
    "\n",
    "One way to verify this claim is by checking if the Concept Frame correlation with the words used\n",
    "to generate is higher than its correlation with other words or random frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/flag-representation-hypothesis/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from frames.representations import FrameUnembeddingRepresentation\n",
    "from frames.utils.memory import garbage_collection_cuda\n",
    "from frames.utils.plotting import histplot_and_save\n",
    "from frames.utils.settings import load_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = load_models()\n",
    "\n",
    "LEMMA_COUNT = {\n",
    "    \"gemma\": 22,\n",
    "    \"llama\": 54,\n",
    "    \"phi\": 15,\n",
    "}  # avoid GPU OOM errors (+80GB required)\n",
    "FLAG_RANK = 3\n",
    "N_RANDOM = 100\n",
    "\n",
    "X = \"total projection\"\n",
    "HUE = \"model\"\n",
    "\n",
    "RAND_PROJ = \"Random Projection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(concepts, words):\n",
    "    sim = concepts.similarity(words)\n",
    "    return sim[sim != 0]  # remove padded null vectors and flatten\n",
    "\n",
    "\n",
    "def compare_words_and_random_frames(*args, **kwargs):\n",
    "    garbage_collection_cuda()\n",
    "\n",
    "    fur = FrameUnembeddingRepresentation.from_model_id(*args, **kwargs)\n",
    "\n",
    "    kw = dict(\n",
    "        min_lemmas_per_synset=next(\n",
    "            v for k, v in LEMMA_COUNT.items() if k in kwargs[\"id\"].lower()\n",
    "        ),\n",
    "        max_token_count=FLAG_RANK,\n",
    "    )\n",
    "\n",
    "    all_concepts = fur.get_all_concepts(**kw)\n",
    "    print(\"Concepts =\", all_concepts)\n",
    "\n",
    "    all_word_frames = fur.get_all_words_frames(**kw)\n",
    "    print(\"Words =\", all_concepts)\n",
    "\n",
    "    random_word_frames = torch.rand_like(all_word_frames.tensor[:N_RANDOM])\n",
    "\n",
    "    concept_word_sim = compute_similarities(all_concepts, all_word_frames).float()\n",
    "    concept_random_sim = compute_similarities(all_concepts, random_word_frames).float()\n",
    "\n",
    "    rand_df = pd.DataFrame({X: concept_random_sim.cpu()}).assign(**{HUE: RAND_PROJ})\n",
    "    word_df = pd.DataFrame({X: concept_word_sim.cpu()}).assign(**{HUE: str(fur.model)})\n",
    "\n",
    "    return pd.concat([rand_df, word_df])\n",
    "\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        compare_words_and_random_frames(**kwargs)\n",
    "        for kwargs in MODELS.to_dict(orient=\"index\").values()\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "histplot_and_save(\"03_concept_vs_word_frames_relationship\", df, X, HUE, discrete=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
